{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95db19c1-8ecc-4252-8e52-3b539b794e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "X = pd.read_parquet('df_X.parquet.gzip') \n",
    "df_subm = pd.read_csv('sample_submission.csv', index_col='id')\n",
    "X_test = pd.read_parquet('df_test.parquet.gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce86e963-0dfb-4da8-b0c0-79932a2e4e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Episode_Length_minutes_Episode_Title',\n",
       "       'Episode_Length_minutes_Host_Popularity_percentage',\n",
       "       'Episode_Length_minutes_Number_of_Ads',\n",
       "       'Episode_Length_minutes_Episode_Sentiment',\n",
       "       'Episode_Length_minutes_Publication_Day',\n",
       "       'Episode_Length_minutes_Publication_Time',\n",
       "       'Episode_Length_minutes_Podcast_Name',\n",
       "       'Episode_Length_minutes_Guest_Popularity_percentage',\n",
       "       'Episode_Length_minutes_Genre',\n",
       "       'Episode_Title_Host_Popularity_percentage',\n",
       "       ...\n",
       "       'Episode_Sentiment_Publication_Day_Guest_Popularity_percentage_Genre',\n",
       "       'Episode_Sentiment_Publication_Time_Podcast_Name_Guest_Popularity_percentage',\n",
       "       'Episode_Sentiment_Publication_Time_Podcast_Name_Genre',\n",
       "       'Episode_Sentiment_Publication_Time_Guest_Popularity_percentage_Genre',\n",
       "       'Episode_Sentiment_Podcast_Name_Guest_Popularity_percentage_Genre',\n",
       "       'Publication_Day_Publication_Time_Podcast_Name_Guest_Popularity_percentage',\n",
       "       'Publication_Day_Publication_Time_Podcast_Name_Genre',\n",
       "       'Publication_Day_Publication_Time_Guest_Popularity_percentage_Genre',\n",
       "       'Publication_Day_Podcast_Name_Guest_Popularity_percentage_Genre',\n",
       "       'Publication_Time_Podcast_Name_Guest_Popularity_percentage_Genre'],\n",
       "      dtype='object', length=375)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X['Listening_Time_minutes']\n",
    "TE_col = X.columns[207:]\n",
    "TE_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea27370c-a5d6-4b15-accc-0fb9a78f245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158974"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(5, random_state=42, shuffle=True)\n",
    "idx = list(cv.split(X, y))\n",
    "len(idx[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f96546c-7f60-4a82-a2c3-19d09a8102a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 45/45 [00:00<00:00, 1497965.71it/s]\n",
      "100%|████████████████████████████████████| 120/120 [00:00<00:00, 3050402.91it/s]\n"
     ]
    }
   ],
   "source": [
    "encode_columns = ['Episode_Length_minutes', 'Episode_Title', 'Host_Popularity_percentage', 'Number_of_Ads'\n",
    "                  ,'Episode_Sentiment', 'Publication_Day', 'Publication_Time','Podcast_Name','Guest_Popularity_percentage'\n",
    "                 ,'Genre']\n",
    "TE2 = []\n",
    "for cols in tqdm(list(combinations(encode_columns, 2))):\n",
    "        new_col_name = '_'.join(cols)\n",
    "        TE2.append(new_col_name)\n",
    "\n",
    "encode_columns = ['Episode_Length_minutes', 'Episode_Title', 'Host_Popularity_percentage', 'Number_of_Ads'\n",
    "                  ,'Episode_Sentiment', 'Publication_Day', 'Publication_Time','Podcast_Name','Guest_Popularity_percentage'\n",
    "                 ,'Genre']\n",
    "TE3 = []\n",
    "for cols in tqdm(list(combinations(encode_columns, 3))):\n",
    "        new_col_name = '_'.join(cols)\n",
    "        TE3.append(new_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657d88c0-36c6-41c7-9b03-c203e7295ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_eng(idx_train,idx_valid):\n",
    "    global X_test, X\n",
    "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "    for x in [0.05,0.10,0.40,0.45,0.55,0.60,0.90,0.95]:\n",
    "        for w in ['Listening_Time_minutes']:\n",
    "            for z in ['Publication_Day','Podcast_Name','Publication_Time','Episode_Title','Genre','NaNs'\n",
    "                      , 'Episode_Length_minutes_round0','Host_Popularity_percentage_round0','Guest_Popularity_percentage_round0'\n",
    "                      , 'Episode_Length_minutes_digit0','Episode_Length_minutes_digit1', 'Host_Popularity_percentage_digit0'\n",
    "                      ,'Host_Popularity_percentage_digit1','Guest_Popularity_percentage_digit0','Guest_Popularity_percentage_digit1']:\n",
    "                name = w + str(x) + z\n",
    "                a = X_train.groupby(z)[w].quantile(x).reset_index(name=name)\n",
    "                X_train = X_train.merge(a,on=z,how='left')\n",
    "                X_test = X_test.merge(a,on=z,how='left')      \n",
    "                X_valid = X_valid.merge(a,on=z,how='left') \n",
    "    print(X_valid.shape,len(y_valid))\n",
    "    print('2')\n",
    "\n",
    "    for x in ['mean','max','min','std','quantile','skew','sum']:\n",
    "        for w in ['Episode_Length_minutes']:\n",
    "            for z in ['Publication_Day','Podcast_Name','Publication_Time','Episode_Title','Genre','Number_of_Ads']:\n",
    "                name = w + x + z\n",
    "                a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "                X_train = X_train.merge(a,on=z,how='left')\n",
    "                X_test = X_test.merge(a,on=z,how='left')      \n",
    "                X_valid = X_valid.merge(a,on=z,how='left') \n",
    "    print(X_valid.shape,len(y_valid))\n",
    "    print(X_train.shape,len(y_valid))\n",
    "    print('1')\n",
    "\n",
    "    for x in [5,10,15,20,25,50,100,150]:\n",
    "        name = 'Episode_Length_minutes' + str(x)\n",
    "        X_train[\"Episode_Length_minutes_copy\"] = X_train[\"Episode_Length_minutes\"].copy()\n",
    "        X_valid[\"Episode_Length_minutes_copy\"] = X_valid[\"Episode_Length_minutes\"].copy()\n",
    "        X_test[\"Episode_Length_minutes_copy\"] = X_test[\"Episode_Length_minutes\"].copy()\n",
    "        \n",
    "        ser, bins = pd.qcut(X_train[\"Episode_Length_minutes_copy\"], x, retbins=True, labels=False)\n",
    "        X_train[name] = pd.cut(X_train['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "        X_valid[name] = pd.cut(X_valid['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "        X_test[name] = pd.cut(X_test['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    \n",
    "    for x in ['mean','quantile']:\n",
    "        for w in ['Listening_Time_minutes']:\n",
    "            for z in ['Episode_Length_minutes5','Episode_Length_minutes10','Episode_Length_minutes15','Episode_Length_minutes20'\n",
    "                     ,'Episode_Length_minutes25','Episode_Length_minutes50','Episode_Length_minutes100','Episode_Length_minutes150']:\n",
    "                name = w + x + z\n",
    "                a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "                X_train = X_train.merge(a,on=z,how='left')\n",
    "                X_test = X_test.merge(a,on=z,how='left')      \n",
    "                X_valid = X_valid.merge(a,on=z,how='left') \n",
    "    print(X_valid.shape,len(y_valid))\n",
    "    print('8')\n",
    "\n",
    "    for x in ['count']:\n",
    "        for w in ['Publication_Day','Podcast_Name','Publication_Time','Episode_Title','Genre']:\n",
    "            name = w + x\n",
    "            a = X_train.reset_index().groupby(w)['index'].apply(x).reset_index(name=name)\n",
    "            X_train = X_train.merge(a,on=w)\n",
    "            X_test = X_test.merge(a,on=w)\n",
    "            X_valid = X_valid.merge(a,on=w)   \n",
    "    print(X_valid.shape,len(y_valid))\n",
    "    print('5')\n",
    "    \n",
    "    print('6')\n",
    "    return X_train,X_valid,y_train,y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720c0df-3162-4f4f-a9a0-f9d3ab34846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in use\n",
    "def Feature_eng(idx_train,idx_valid):\n",
    "    global X_test, X\n",
    "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "    for x in ['mean','max','min','std','quantile','skew','sum']:\n",
    "        for w in ['Listening_Time_minutes']:\n",
    "            for z in ['NaNs', 'Episode_Length_minutes_round0','Host_Popularity_percentage_round0','Guest_Popularity_percentage_round0'\n",
    "                      , 'Episode_Length_minutes_digit0','Episode_Length_minutes_digit1', 'Host_Popularity_percentage_digit0'\n",
    "                      ,'Host_Popularity_percentage_digit1','Guest_Popularity_percentage_digit0','Guest_Popularity_percentage_digit1'\n",
    "                     ,'Publication_Day','Podcast_Name','Publication_Time','Episode_Title','Genre','Number_of_Ads']:\n",
    "                name = w + x + z\n",
    "                a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "                X_train = X_train.merge(a,on=z,how='left')\n",
    "                X_test = X_test.merge(a,on=z,how='left')      \n",
    "                X_valid = X_valid.merge(a,on=z,how='left')\n",
    "    print(X_valid.shape,len(y_valid))\n",
    "\n",
    "    encoded_columns = TE_col\n",
    "    encoder = TargetEncoder(random_state=42)\n",
    "    \n",
    "    X_train[encoded_columns] = encoder.fit_transform(X_train[encoded_columns], y_train)\n",
    "    X_valid[encoded_columns] = encoder.transform(X_valid[encoded_columns])\n",
    "    X_test[encoded_columns] = encoder.transform(X_test[encoded_columns])\n",
    "    print(X_valid.shape,len(y_valid))\n",
    "\n",
    "    for x in TE2:\n",
    "        a = 'Episode_Length_minutes' + '_div_' + x\n",
    "        X_train[a] = X_train['Episode_Length_minutes'] / X_train[x]\n",
    "        X_valid[a] = X_valid['Episode_Length_minutes'] / X_valid[x]\n",
    "        X_test[a] = X_test['Episode_Length_minutes'] / X_test[x]\n",
    "    print(X_valid.shape,len(y_valid))\n",
    "    \n",
    "    for x in TE3:\n",
    "        a = 'Episode_Length_minutes' + '_div_' + x\n",
    "        X_train[a] = X_train['Episode_Length_minutes'] / X_train[x]\n",
    "        X_valid[a] = X_valid['Episode_Length_minutes'] / X_valid[x]\n",
    "        X_test[a] = X_test['Episode_Length_minutes'] / X_test[x]\n",
    "    print(X_valid.shape,len(y_valid))\n",
    "    \n",
    "    X_train['SinEpLen'] = np.sin(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "    X_train['CosEpLen'] = np.cos(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "    X_train[\"ELen_Int\"] = np.floor(X_train[\"Episode_Length_minutes\"])\n",
    "    X_train[\"ELen_Dec\"] = X_train[\"Episode_Length_minutes\"] - X_train[\"ELen_Int\"]\n",
    "    \n",
    "    X_test['SinEpLen'] = np.sin(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "    X_test['CosEpLen'] = np.cos(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "    X_test[\"ELen_Int\"] = np.floor(X_test[\"Episode_Length_minutes\"])\n",
    "    X_test[\"ELen_Dec\"] = X_test[\"Episode_Length_minutes\"] - X_test[\"ELen_Int\"]\n",
    "    \n",
    "    X_valid['SinEpLen'] = np.sin(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "    X_valid['CosEpLen'] = np.cos(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "    X_valid[\"ELen_Int\"] = np.floor(X_valid[\"Episode_Length_minutes\"])\n",
    "    X_valid[\"ELen_Dec\"] = X_valid[\"Episode_Length_minutes\"] - X_valid[\"ELen_Int\"]\n",
    "    print(X_valid.shape,len(y_valid))\n",
    "    return X_train,X_valid,y_train,y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0541a64d-ff71-414b-8f06-e2aaca84515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158974, 694) 158974\n",
      "(158974, 694) 158974\n",
      "(158974, 739) 158974\n",
      "(158974, 859) 158974\n",
      "(158974, 863) 158974\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "idx_train = idx[0][0]\n",
    "idx_valid = idx[0][1]\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = Feature_eng(idx_train,idx_valid)\n",
    "\n",
    "X_train.to_parquet('X_train_1.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_valid.to_parquet('X_valid_1.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_test.to_parquet('X_test_1.parquet.gzip',\n",
    "          compression='gzip') \n",
    "y_train.to_csv('y_train_1.csv') \n",
    "y_valid.to_csv('y_valid_1.csv') \n",
    "print('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "550acc4a-34fb-473e-a560-081f780f1aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158974, 636) 158974\n",
      "8\n",
      "(158974, 671) 158974\n",
      "1\n",
      "(158974, 791) 158974\n",
      "2\n",
      "(158974, 833) 158974\n",
      "(635893, 833) 158974\n",
      "1\n",
      "(158974, 838) 158974\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "idx_train = idx[1][0]\n",
    "idx_valid = idx[1][1]\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = Feature_eng(idx_train,idx_valid)\n",
    "\n",
    "X_train.to_parquet('X_train_2.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_valid.to_parquet('X_valid_2.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_test.to_parquet('X_test_2.parquet.gzip',\n",
    "          compression='gzip') \n",
    "y_train.to_csv('y_train_2.csv') \n",
    "y_valid.to_csv('y_valid_2.csv') \n",
    "print('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf334a2-a02c-4590-8a5a-c5d2ca16b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158973, 636) 158973\n",
      "8\n",
      "(158973, 671) 158973\n",
      "1\n",
      "(158973, 791) 158973\n",
      "2\n",
      "(158973, 833) 158973\n",
      "(635894, 833) 158973\n",
      "1\n",
      "(158973, 838) 158973\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "idx_train = idx[2][0]\n",
    "idx_valid = idx[2][1]\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = Feature_eng(idx_train,idx_valid)\n",
    "\n",
    "X_train.to_parquet('X_train_3.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_valid.to_parquet('X_valid_3.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_test.to_parquet('X_test_3.parquet.gzip',\n",
    "          compression='gzip') \n",
    "y_train.to_csv('y_train_3.csv') \n",
    "y_valid.to_csv('y_valid_3.csv') \n",
    "print('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc3c5e9-1f3e-4ead-a39b-685584447dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158973, 636) 158973\n",
      "8\n",
      "(158973, 671) 158973\n",
      "1\n",
      "(158973, 791) 158973\n",
      "2\n",
      "(158973, 833) 158973\n",
      "(635894, 833) 158973\n",
      "1\n",
      "(158973, 838) 158973\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "idx_train = idx[3][0]\n",
    "idx_valid = idx[3][1]\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = Feature_eng(idx_train,idx_valid)\n",
    "\n",
    "X_train.to_parquet('X_train_4.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_valid.to_parquet('X_valid_4.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_test.to_parquet('X_test_4.parquet.gzip',\n",
    "          compression='gzip') \n",
    "y_train.to_csv('y_train_4.csv') \n",
    "y_valid.to_csv('y_valid_4.csv') \n",
    "print('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1920f5-c72a-4151-8f68-7e88699e7ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158973, 636) 158973\n",
      "8\n",
      "(158973, 671) 158973\n",
      "1\n",
      "(158973, 791) 158973\n",
      "2\n",
      "(158973, 833) 158973\n",
      "(635894, 833) 158973\n",
      "1\n",
      "(158973, 838) 158973\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "idx_train = idx[4][0]\n",
    "idx_valid = idx[4][1]\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = Feature_eng(idx_train,idx_valid)\n",
    "\n",
    "X_train.to_parquet('X_train_5.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_valid.to_parquet('X_valid_5.parquet.gzip',\n",
    "          compression='gzip') \n",
    "X_test.to_parquet('X_test_5.parquet.gzip',\n",
    "          compression='gzip') \n",
    "y_train.to_csv('y_train_5.csv') \n",
    "y_valid.to_csv('y_valid_5.csv') \n",
    "print('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3934cbd0-1dde-4b80-b8da-d03a97085159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

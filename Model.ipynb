{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm = pd.read_csv('sample_submission.csv', index_col='id')\n",
    "y_pred = np.zeros(len(df_subm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_pred):\n",
    "    model = lgb.LGBMRegressor(\n",
    "    n_iter=50000,\n",
    "    max_depth=-1,\n",
    "    num_leaves=1024,\n",
    "    colsample_bytree=0.2,\n",
    "    subsample = 0.7,\n",
    "    learning_rate=0.005,\n",
    "    objective='l2',\n",
    "    metric='rmse', \n",
    "    verbosity=-1,\n",
    "    max_bin=1024,\n",
    "    random_state = 42\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train,y_train),(X_valid, y_valid)],\n",
    "        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(200)],\n",
    "    )\n",
    "\n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "    y_oof = model.predict(X_test)\n",
    "    \n",
    "    y_pred += y_oof\n",
    "    return y_oof, y_pred_valid,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(158974, 872) 158974\n",
      "(158974, 880) 158974\n",
      "(158974, 952) 158974\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 18.9847\tvalid_1's rmse: 19.211\n",
      "[200]\ttraining's rmse: 14.7023\tvalid_1's rmse: 15.1814\n",
      "[300]\ttraining's rmse: 12.5691\tvalid_1's rmse: 13.3437\n",
      "[400]\ttraining's rmse: 11.4997\tvalid_1's rmse: 12.558\n",
      "[500]\ttraining's rmse: 10.9139\tvalid_1's rmse: 12.2269\n",
      "[600]\ttraining's rmse: 10.5417\tvalid_1's rmse: 12.0823\n",
      "[700]\ttraining's rmse: 10.2655\tvalid_1's rmse: 12.0127\n",
      "[800]\ttraining's rmse: 10.0372\tvalid_1's rmse: 11.977\n",
      "[900]\ttraining's rmse: 9.8353\tvalid_1's rmse: 11.9558\n"
     ]
    }
   ],
   "source": [
    "#training's rmse: 4.01842\tvalid_1's rmse: 11.8759\n",
    "##training's rmse: 5.46618\tvalid_1's rmse: 11.8854\n",
    "X_train = pd.read_parquet('X_train_1.parquet.gzip') \n",
    "X_valid = pd.read_parquet('X_valid_1.parquet.gzip')\n",
    "y_train = pd.read_csv('y_train_1.csv') \n",
    "y_valid = pd.read_csv('y_valid_1.csv')\n",
    "X_test = pd.read_parquet('X_test_1.parquet.gzip')\n",
    "\n",
    "y_train = y_train['Listening_Time_minutes']\n",
    "y_valid = y_valid['Listening_Time_minutes']\n",
    "\n",
    "X_train['Genre'] = X_train['Genre'].astype('category')\n",
    "X_train['Podcast_Name'] = X_train['Podcast_Name'].astype('category')\n",
    "X_train['Publication_Day'] = X_train['Publication_Day'].astype('category')\n",
    "X_train['Publication_Time'] = X_train['Publication_Time'].astype('category')\n",
    "X_train['Episode_Sentiment'] = X_train['Episode_Sentiment'].astype('category')\n",
    "X_train['Episode_Title'] = X_train['Episode_Title'].astype('category')\n",
    "\n",
    "X_valid['Genre'] = X_valid['Genre'].astype('category')\n",
    "X_valid['Podcast_Name'] = X_valid['Podcast_Name'].astype('category')\n",
    "X_valid['Publication_Day'] = X_valid['Publication_Day'].astype('category')\n",
    "X_valid['Publication_Time'] = X_valid['Publication_Time'].astype('category')\n",
    "X_valid['Episode_Sentiment'] = X_valid['Episode_Sentiment'].astype('category')\n",
    "X_valid['Episode_Title'] = X_valid['Episode_Title'].astype('category')\n",
    "\n",
    "X_test['Genre'] = X_test['Genre'].astype('category')\n",
    "X_test['Podcast_Name'] = X_test['Podcast_Name'].astype('category')\n",
    "X_test['Publication_Day'] = X_test['Publication_Day'].astype('category')\n",
    "X_test['Publication_Time'] = X_test['Publication_Time'].astype('category')\n",
    "X_test['Episode_Sentiment'] = X_test['Episode_Sentiment'].astype('category')\n",
    "X_test['Episode_Title'] = X_test['Episode_Title'].astype('category')\n",
    "print('1')\n",
    "\n",
    "for x in [5,10,15,20,25,50,100,150]:\n",
    "    name = 'Episode_Length_minutes' + str(x)\n",
    "    X_train[\"Episode_Length_minutes_copy\"] = X_train[\"Episode_Length_minutes\"].copy()\n",
    "    X_valid[\"Episode_Length_minutes_copy\"] = X_valid[\"Episode_Length_minutes\"].copy()\n",
    "    X_test[\"Episode_Length_minutes_copy\"] = X_test[\"Episode_Length_minutes\"].copy()\n",
    "    \n",
    "    ser, bins = pd.qcut(X_train[\"Episode_Length_minutes_copy\"], x, retbins=True, labels=False)\n",
    "    X_train[name] = pd.cut(X_train['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_valid[name] = pd.cut(X_valid['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_test[name] = pd.cut(X_test['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "print(X_valid.shape,len(y_valid))\n",
    "\n",
    "for x in ['mean']:\n",
    "    for w in ['Listening_Time_minutes']:\n",
    "        for z in ['Episode_Length_minutes5','Episode_Length_minutes10','Episode_Length_minutes15','Episode_Length_minutes20'\n",
    "                 ,'Episode_Length_minutes25','Episode_Length_minutes50','Episode_Length_minutes100','Episode_Length_minutes150']:\n",
    "            name = w + x + z\n",
    "            a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "            X_train = X_train.merge(a,on=z,how='left')\n",
    "            X_test = X_test.merge(a,on=z,how='left')      \n",
    "            X_valid = X_valid.merge(a,on=z,how='left') \n",
    "print(X_valid.shape,len(y_valid))\n",
    "\n",
    "QUANTILE_LIST = []\n",
    "for x in [0.05,0.10,0.40,0.45,0.55,0.60,0.90,0.95]:\n",
    "    for w in ['Listening_Time_minutes']:\n",
    "        for z in ['Publication_Day','Podcast_Name','Publication_Time','Episode_Title','Genre','NaNs'\n",
    "                  , 'Episode_Length_minutes_round0', 'Episode_Length_minutes_digit0','Episode_Length_minutes_digit1']:\n",
    "            name = w + str(x) + z\n",
    "            a = X_train.groupby(z)[w].quantile(x).reset_index(name=name)\n",
    "            X_train = X_train.merge(a,on=z,how='left')\n",
    "            X_test = X_test.merge(a,on=z,how='left')      \n",
    "            X_valid = X_valid.merge(a,on=z,how='left')\n",
    "            QUANTILE_LIST.append(name)\n",
    "print(X_valid.shape,len(y_valid))\n",
    "\n",
    "X_train = X_train.drop(columns=['Listening_Time_minutes'])\n",
    "X_valid = X_valid.drop(columns=['Listening_Time_minutes'])\n",
    "\n",
    "y_oof, y_pred_valid,model_1 = train(X_train,y_train,X_test,y_pred)\n",
    "np.savetxt(\"y_oof_1.csv\", y_oof, delimiter=\",\",header='Listening_Time_minutes')\n",
    "np.savetxt(\"y_pred_valid_1.csv\", y_pred_valid, delimiter=\",\",header='Listening_Time_minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "lgb.plot_importance(model_1,max_num_features=50,importance_type='gain',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training's rmse: 4.61699\tvalid_1's rmse: 11.8012\n",
    "X_train = pd.read_parquet('X_train_2.parquet.gzip') \n",
    "X_valid = pd.read_parquet('X_valid_2.parquet.gzip')\n",
    "y_train = pd.read_csv('y_train_2.csv') \n",
    "y_valid = pd.read_csv('y_valid_2.csv')\n",
    "X_test = pd.read_parquet('X_test_2.parquet.gzip')\n",
    "\n",
    "y_train = y_train['Listening_Time_minutes']\n",
    "y_valid = y_valid['Listening_Time_minutes']\n",
    "\n",
    "X_train['Genre'] = X_train['Genre'].astype('category')\n",
    "X_train['Podcast_Name'] = X_train['Podcast_Name'].astype('category')\n",
    "X_train['Publication_Day'] = X_train['Publication_Day'].astype('category')\n",
    "X_train['Publication_Time'] = X_train['Publication_Time'].astype('category')\n",
    "X_train['Episode_Sentiment'] = X_train['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_valid['Genre'] = X_valid['Genre'].astype('category')\n",
    "X_valid['Podcast_Name'] = X_valid['Podcast_Name'].astype('category')\n",
    "X_valid['Publication_Day'] = X_valid['Publication_Day'].astype('category')\n",
    "X_valid['Publication_Time'] = X_valid['Publication_Time'].astype('category')\n",
    "X_valid['Episode_Sentiment'] = X_valid['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_test['Genre'] = X_test['Genre'].astype('category')\n",
    "X_test['Podcast_Name'] = X_test['Podcast_Name'].astype('category')\n",
    "X_test['Publication_Day'] = X_test['Publication_Day'].astype('category')\n",
    "X_test['Publication_Time'] = X_test['Publication_Time'].astype('category')\n",
    "X_test['Episode_Sentiment'] = X_test['Episode_Sentiment'].astype('category')\n",
    "\n",
    "for x in X_train.columns[191:-5]:\n",
    "    a = x + 'div' + 'Episode_Length_minutes'\n",
    "    X_train[a] = X_train['Episode_Length_minutes'] / X_train[x]\n",
    "    X_valid[a] = X_valid['Episode_Length_minutes'] / X_valid[x]\n",
    "    X_test[a] = X_test['Episode_Length_minutes'] / X_test[x]\n",
    "\n",
    "for x in [5,10,15,20,25,50,100]:\n",
    "    name = 'Episode_Length_minutes' + str(x)\n",
    "    X_train[\"Episode_Length_minutes_copy\"] = X_train[\"Episode_Length_minutes\"].copy()\n",
    "    X_valid[\"Episode_Length_minutes_copy\"] = X_valid[\"Episode_Length_minutes\"].copy()\n",
    "    X_test[\"Episode_Length_minutes_copy\"] = X_test[\"Episode_Length_minutes\"].copy()\n",
    "    \n",
    "    ser, bins = pd.qcut(X_train[\"Episode_Length_minutes_copy\"], x, retbins=True, labels=False)\n",
    "    X_train[name] = pd.cut(X_train['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_valid[name] = pd.cut(X_valid['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_test[name] = pd.cut(X_test['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "\n",
    "for x in ['mean','quantile']:\n",
    "    for w in ['Listening_Time_minutes']:\n",
    "        for z in ['Episode_Length_minutes5','Episode_Length_minutes10','Episode_Length_minutes15','Episode_Length_minutes20'\n",
    "                 ,'Episode_Length_minutes25','Episode_Length_minutes50','Episode_Length_minutes100']:\n",
    "            name = w + x + z\n",
    "            a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "            X_train = X_train.merge(a,on=z,how='left')\n",
    "            X_test = X_test.merge(a,on=z,how='left')      \n",
    "            X_valid = X_valid.merge(a,on=z,how='left') \n",
    "print(X_valid.shape,len(y_valid))\n",
    "print('8')\n",
    "\n",
    "X_train['SinEpLen'] = np.sin(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train['CosEpLen'] = np.cos(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train[\"ELen_Int\"] = np.floor(X_train[\"Episode_Length_minutes\"])\n",
    "X_train[\"ELen_Dec\"] = X_train[\"Episode_Length_minutes\"] - X_train[\"ELen_Int\"]\n",
    "X_train['Long_Episode'] = (X_train['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_test['SinEpLen'] = np.sin(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test['CosEpLen'] = np.cos(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test[\"ELen_Int\"] = np.floor(X_test[\"Episode_Length_minutes\"])\n",
    "X_test[\"ELen_Dec\"] = X_test[\"Episode_Length_minutes\"] - X_test[\"ELen_Int\"]\n",
    "X_test['Long_Episode'] = (X_test['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_valid['SinEpLen'] = np.sin(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid['CosEpLen'] = np.cos(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid[\"ELen_Int\"] = np.floor(X_valid[\"Episode_Length_minutes\"])\n",
    "X_valid[\"ELen_Dec\"] = X_valid[\"Episode_Length_minutes\"] - X_valid[\"ELen_Int\"]\n",
    "X_valid['Long_Episode'] = (X_valid['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_train = X_train.drop(columns=['Listening_Time_minutes'])\n",
    "X_valid = X_valid.drop(columns=['Listening_Time_minutes'])\n",
    "\n",
    "y_oof, y_pred_valid,model_2 = train(X_train,y_train,X_test,y_pred)\n",
    "np.savetxt(\"y_oof_2.csv\", y_oof, delimiter=\",\",header='Listening_Time_minutes')\n",
    "np.savetxt(\"y_pred_valid_2.csv\", y_pred_valid, delimiter=\",\",header='Listening_Time_minutes')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training's rmse: 5.64439\tvalid_1's rmse: 11.8389\n",
    "X_train = pd.read_parquet('X_train_3.parquet.gzip') \n",
    "X_valid = pd.read_parquet('X_valid_3.parquet.gzip')\n",
    "y_train = pd.read_csv('y_train_3.csv') \n",
    "y_valid = pd.read_csv('y_valid_3.csv')\n",
    "X_test = pd.read_parquet('X_test_3.parquet.gzip')\n",
    "\n",
    "y_train = y_train['Listening_Time_minutes']\n",
    "y_valid = y_valid['Listening_Time_minutes']\n",
    "\n",
    "X_train['Genre'] = X_train['Genre'].astype('category')\n",
    "X_train['Podcast_Name'] = X_train['Podcast_Name'].astype('category')\n",
    "X_train['Publication_Day'] = X_train['Publication_Day'].astype('category')\n",
    "X_train['Publication_Time'] = X_train['Publication_Time'].astype('category')\n",
    "X_train['Episode_Sentiment'] = X_train['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_valid['Genre'] = X_valid['Genre'].astype('category')\n",
    "X_valid['Podcast_Name'] = X_valid['Podcast_Name'].astype('category')\n",
    "X_valid['Publication_Day'] = X_valid['Publication_Day'].astype('category')\n",
    "X_valid['Publication_Time'] = X_valid['Publication_Time'].astype('category')\n",
    "X_valid['Episode_Sentiment'] = X_valid['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_test['Genre'] = X_test['Genre'].astype('category')\n",
    "X_test['Podcast_Name'] = X_test['Podcast_Name'].astype('category')\n",
    "X_test['Publication_Day'] = X_test['Publication_Day'].astype('category')\n",
    "X_test['Publication_Time'] = X_test['Publication_Time'].astype('category')\n",
    "X_test['Episode_Sentiment'] = X_test['Episode_Sentiment'].astype('category')\n",
    "\n",
    "for x in X_train.columns[191:-5]:\n",
    "    a = x + 'div' + 'Episode_Length_minutes'\n",
    "    X_train[a] = X_train['Episode_Length_minutes'] / X_train[x]\n",
    "    X_valid[a] = X_valid['Episode_Length_minutes'] / X_valid[x]\n",
    "    X_test[a] = X_test['Episode_Length_minutes'] / X_test[x]\n",
    "\n",
    "for x in [5,10,15,20,25,50,100]:\n",
    "    name = 'Episode_Length_minutes' + str(x)\n",
    "    X_train[\"Episode_Length_minutes_copy\"] = X_train[\"Episode_Length_minutes\"].copy()\n",
    "    X_valid[\"Episode_Length_minutes_copy\"] = X_valid[\"Episode_Length_minutes\"].copy()\n",
    "    X_test[\"Episode_Length_minutes_copy\"] = X_test[\"Episode_Length_minutes\"].copy()\n",
    "    \n",
    "    ser, bins = pd.qcut(X_train[\"Episode_Length_minutes_copy\"], x, retbins=True, labels=False)\n",
    "    X_train[name] = pd.cut(X_train['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_valid[name] = pd.cut(X_valid['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_test[name] = pd.cut(X_test['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "\n",
    "for x in ['mean','quantile']:\n",
    "    for w in ['Listening_Time_minutes']:\n",
    "        for z in ['Episode_Length_minutes5','Episode_Length_minutes10','Episode_Length_minutes15','Episode_Length_minutes20'\n",
    "                 ,'Episode_Length_minutes25','Episode_Length_minutes50','Episode_Length_minutes100']:\n",
    "            name = w + x + z\n",
    "            a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "            X_train = X_train.merge(a,on=z,how='left')\n",
    "            X_test = X_test.merge(a,on=z,how='left')      \n",
    "            X_valid = X_valid.merge(a,on=z,how='left') \n",
    "print(X_valid.shape,len(y_valid))\n",
    "print('8')\n",
    "\n",
    "X_train['SinEpLen'] = np.sin(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train['CosEpLen'] = np.cos(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train[\"ELen_Int\"] = np.floor(X_train[\"Episode_Length_minutes\"])\n",
    "X_train[\"ELen_Dec\"] = X_train[\"Episode_Length_minutes\"] - X_train[\"ELen_Int\"]\n",
    "X_train['Long_Episode'] = (X_train['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_test['SinEpLen'] = np.sin(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test['CosEpLen'] = np.cos(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test[\"ELen_Int\"] = np.floor(X_test[\"Episode_Length_minutes\"])\n",
    "X_test[\"ELen_Dec\"] = X_test[\"Episode_Length_minutes\"] - X_test[\"ELen_Int\"]\n",
    "X_test['Long_Episode'] = (X_test['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_valid['SinEpLen'] = np.sin(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid['CosEpLen'] = np.cos(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid[\"ELen_Int\"] = np.floor(X_valid[\"Episode_Length_minutes\"])\n",
    "X_valid[\"ELen_Dec\"] = X_valid[\"Episode_Length_minutes\"] - X_valid[\"ELen_Int\"]\n",
    "X_valid['Long_Episode'] = (X_valid['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_train = X_train.drop(columns=['Listening_Time_minutes'])\n",
    "X_valid = X_valid.drop(columns=['Listening_Time_minutes'])\n",
    "\n",
    "y_oof, y_pred_valid,model_3 = train(X_train,y_train,X_test,y_pred)\n",
    "np.savetxt(\"y_oof_3.csv\", y_oof, delimiter=\",\",header='Listening_Time_minutes')\n",
    "np.savetxt(\"y_pred_valid_3.csv\", y_pred_valid, delimiter=\",\",header='Listening_Time_minutes')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model_3,max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training's rmse: 4.70523\tvalid_1's rmse: 11.8398\n",
    "X_train = pd.read_parquet('X_train_4.parquet.gzip') \n",
    "X_valid = pd.read_parquet('X_valid_4.parquet.gzip')\n",
    "y_train = pd.read_csv('y_train_4.csv') \n",
    "y_valid = pd.read_csv('y_valid_4.csv')\n",
    "X_test = pd.read_parquet('X_test_4.parquet.gzip')\n",
    "\n",
    "y_train = y_train['Listening_Time_minutes']\n",
    "y_valid = y_valid['Listening_Time_minutes']\n",
    "\n",
    "X_train['Genre'] = X_train['Genre'].astype('category')\n",
    "X_train['Podcast_Name'] = X_train['Podcast_Name'].astype('category')\n",
    "X_train['Publication_Day'] = X_train['Publication_Day'].astype('category')\n",
    "X_train['Publication_Time'] = X_train['Publication_Time'].astype('category')\n",
    "X_train['Episode_Sentiment'] = X_train['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_valid['Genre'] = X_valid['Genre'].astype('category')\n",
    "X_valid['Podcast_Name'] = X_valid['Podcast_Name'].astype('category')\n",
    "X_valid['Publication_Day'] = X_valid['Publication_Day'].astype('category')\n",
    "X_valid['Publication_Time'] = X_valid['Publication_Time'].astype('category')\n",
    "X_valid['Episode_Sentiment'] = X_valid['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_test['Genre'] = X_test['Genre'].astype('category')\n",
    "X_test['Podcast_Name'] = X_test['Podcast_Name'].astype('category')\n",
    "X_test['Publication_Day'] = X_test['Publication_Day'].astype('category')\n",
    "X_test['Publication_Time'] = X_test['Publication_Time'].astype('category')\n",
    "X_test['Episode_Sentiment'] = X_test['Episode_Sentiment'].astype('category')\n",
    "\n",
    "for x in X_train.columns[191:-5]:\n",
    "    a = x + 'div' + 'Episode_Length_minutes'\n",
    "    X_train[a] = X_train['Episode_Length_minutes'] / X_train[x]\n",
    "    X_valid[a] = X_valid['Episode_Length_minutes'] / X_valid[x]\n",
    "    X_test[a] = X_test['Episode_Length_minutes'] / X_test[x]\n",
    "\n",
    "for x in [5,10,15,20,25,50,100]:\n",
    "    name = 'Episode_Length_minutes' + str(x)\n",
    "    X_train[\"Episode_Length_minutes_copy\"] = X_train[\"Episode_Length_minutes\"].copy()\n",
    "    X_valid[\"Episode_Length_minutes_copy\"] = X_valid[\"Episode_Length_minutes\"].copy()\n",
    "    X_test[\"Episode_Length_minutes_copy\"] = X_test[\"Episode_Length_minutes\"].copy()\n",
    "    \n",
    "    ser, bins = pd.qcut(X_train[\"Episode_Length_minutes_copy\"], x, retbins=True, labels=False)\n",
    "    X_train[name] = pd.cut(X_train['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_valid[name] = pd.cut(X_valid['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_test[name] = pd.cut(X_test['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "\n",
    "for x in ['mean','quantile']:\n",
    "    for w in ['Listening_Time_minutes']:\n",
    "        for z in ['Episode_Length_minutes5','Episode_Length_minutes10','Episode_Length_minutes15','Episode_Length_minutes20'\n",
    "                 ,'Episode_Length_minutes25','Episode_Length_minutes50','Episode_Length_minutes100']:\n",
    "            name = w + x + z\n",
    "            a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "            X_train = X_train.merge(a,on=z,how='left')\n",
    "            X_test = X_test.merge(a,on=z,how='left')      \n",
    "            X_valid = X_valid.merge(a,on=z,how='left') \n",
    "print(X_valid.shape,len(y_valid))\n",
    "print('8')\n",
    "\n",
    "X_train['SinEpLen'] = np.sin(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train['CosEpLen'] = np.cos(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train[\"ELen_Int\"] = np.floor(X_train[\"Episode_Length_minutes\"])\n",
    "X_train[\"ELen_Dec\"] = X_train[\"Episode_Length_minutes\"] - X_train[\"ELen_Int\"]\n",
    "X_train['Long_Episode'] = (X_train['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_test['SinEpLen'] = np.sin(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test['CosEpLen'] = np.cos(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test[\"ELen_Int\"] = np.floor(X_test[\"Episode_Length_minutes\"])\n",
    "X_test[\"ELen_Dec\"] = X_test[\"Episode_Length_minutes\"] - X_test[\"ELen_Int\"]\n",
    "X_test['Long_Episode'] = (X_test['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_valid['SinEpLen'] = np.sin(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid['CosEpLen'] = np.cos(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid[\"ELen_Int\"] = np.floor(X_valid[\"Episode_Length_minutes\"])\n",
    "X_valid[\"ELen_Dec\"] = X_valid[\"Episode_Length_minutes\"] - X_valid[\"ELen_Int\"]\n",
    "X_valid['Long_Episode'] = (X_valid['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_train = X_train.drop(columns=['Listening_Time_minutes'])\n",
    "X_valid = X_valid.drop(columns=['Listening_Time_minutes'])\n",
    "\n",
    "y_oof, y_pred_valid,model_4 = train(X_train,y_train,X_test,y_pred)\n",
    "np.savetxt(\"y_oof_4.csv\", y_oof, delimiter=\",\",header='Listening_Time_minutes')\n",
    "np.savetxt(\"y_pred_valid_4.csv\", y_pred_valid, delimiter=\",\",header='Listening_Time_minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model_4,max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training's rmse: 5.5139\tvalid_1's rmse: 11.8236\n",
    "X_train = pd.read_parquet('X_train_5.parquet.gzip') \n",
    "X_valid = pd.read_parquet('X_valid_5.parquet.gzip')\n",
    "y_train = pd.read_csv('y_train_5.csv') \n",
    "y_valid = pd.read_csv('y_valid_5.csv')\n",
    "X_test = pd.read_parquet('X_test_5.parquet.gzip')\n",
    "\n",
    "y_train = y_train['Listening_Time_minutes']\n",
    "y_valid = y_valid['Listening_Time_minutes']\n",
    "\n",
    "X_train['Genre'] = X_train['Genre'].astype('category')\n",
    "X_train['Podcast_Name'] = X_train['Podcast_Name'].astype('category')\n",
    "X_train['Publication_Day'] = X_train['Publication_Day'].astype('category')\n",
    "X_train['Publication_Time'] = X_train['Publication_Time'].astype('category')\n",
    "X_train['Episode_Sentiment'] = X_train['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_valid['Genre'] = X_valid['Genre'].astype('category')\n",
    "X_valid['Podcast_Name'] = X_valid['Podcast_Name'].astype('category')\n",
    "X_valid['Publication_Day'] = X_valid['Publication_Day'].astype('category')\n",
    "X_valid['Publication_Time'] = X_valid['Publication_Time'].astype('category')\n",
    "X_valid['Episode_Sentiment'] = X_valid['Episode_Sentiment'].astype('category')\n",
    "\n",
    "X_test['Genre'] = X_test['Genre'].astype('category')\n",
    "X_test['Podcast_Name'] = X_test['Podcast_Name'].astype('category')\n",
    "X_test['Publication_Day'] = X_test['Publication_Day'].astype('category')\n",
    "X_test['Publication_Time'] = X_test['Publication_Time'].astype('category')\n",
    "X_test['Episode_Sentiment'] = X_test['Episode_Sentiment'].astype('category')\n",
    "\n",
    "for x in X_train.columns[191:-5]:\n",
    "    a = x + 'div' + 'Episode_Length_minutes'\n",
    "    X_train[a] = X_train['Episode_Length_minutes'] / X_train[x]\n",
    "    X_valid[a] = X_valid['Episode_Length_minutes'] / X_valid[x]\n",
    "    X_test[a] = X_test['Episode_Length_minutes'] / X_test[x]\n",
    "\n",
    "for x in [5,10,15,20,25,50,100]:\n",
    "    name = 'Episode_Length_minutes' + str(x)\n",
    "    X_train[\"Episode_Length_minutes_copy\"] = X_train[\"Episode_Length_minutes\"].copy()\n",
    "    X_valid[\"Episode_Length_minutes_copy\"] = X_valid[\"Episode_Length_minutes\"].copy()\n",
    "    X_test[\"Episode_Length_minutes_copy\"] = X_test[\"Episode_Length_minutes\"].copy()\n",
    "    \n",
    "    ser, bins = pd.qcut(X_train[\"Episode_Length_minutes_copy\"], x, retbins=True, labels=False)\n",
    "    X_train[name] = pd.cut(X_train['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_valid[name] = pd.cut(X_valid['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "    X_test[name] = pd.cut(X_test['Episode_Length_minutes_copy'], bins=bins, labels=False, include_lowest=True).astype('category')\n",
    "\n",
    "for x in ['mean','quantile']:\n",
    "    for w in ['Listening_Time_minutes']:\n",
    "        for z in ['Episode_Length_minutes5','Episode_Length_minutes10','Episode_Length_minutes15','Episode_Length_minutes20'\n",
    "                 ,'Episode_Length_minutes25','Episode_Length_minutes50','Episode_Length_minutes100']:\n",
    "            name = w + x + z\n",
    "            a = X_train.groupby(z)[w].apply(x).reset_index(name=name)\n",
    "            X_train = X_train.merge(a,on=z,how='left')\n",
    "            X_test = X_test.merge(a,on=z,how='left')      \n",
    "            X_valid = X_valid.merge(a,on=z,how='left') \n",
    "print(X_valid.shape,len(y_valid))\n",
    "print('8')\n",
    "\n",
    "X_train['SinEpLen'] = np.sin(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train['CosEpLen'] = np.cos(2*np.pi * X_train['Episode_Length_minutes']/60 )\n",
    "X_train[\"ELen_Int\"] = np.floor(X_train[\"Episode_Length_minutes\"])\n",
    "X_train[\"ELen_Dec\"] = X_train[\"Episode_Length_minutes\"] - X_train[\"ELen_Int\"]\n",
    "X_train['Long_Episode'] = (X_train['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_test['SinEpLen'] = np.sin(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test['CosEpLen'] = np.cos(2*np.pi * X_test['Episode_Length_minutes']/60 )\n",
    "X_test[\"ELen_Int\"] = np.floor(X_test[\"Episode_Length_minutes\"])\n",
    "X_test[\"ELen_Dec\"] = X_test[\"Episode_Length_minutes\"] - X_test[\"ELen_Int\"]\n",
    "X_test['Long_Episode'] = (X_test['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_valid['SinEpLen'] = np.sin(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid['CosEpLen'] = np.cos(2*np.pi * X_valid['Episode_Length_minutes']/60 )\n",
    "X_valid[\"ELen_Int\"] = np.floor(X_valid[\"Episode_Length_minutes\"])\n",
    "X_valid[\"ELen_Dec\"] = X_valid[\"Episode_Length_minutes\"] - X_valid[\"ELen_Int\"]\n",
    "X_valid['Long_Episode'] = (X_valid['Episode_Length_minutes'] > 75).astype(int)\n",
    "\n",
    "X_train = X_train.drop(columns=['Listening_Time_minutes'])\n",
    "X_valid = X_valid.drop(columns=['Listening_Time_minutes'])\n",
    "\n",
    "y_oof, y_pred_valid,model_5 = train(X_train,y_train,X_test,y_pred)\n",
    "np.savetxt(\"y_oof_5.csv\", y_oof, delimiter=\",\",header='Listening_Time_minutes')\n",
    "np.savetxt(\"y_pred_valid_5.csv\", y_pred_valid, delimiter=\",\",header='Listening_Time_minutes')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm = pd.read_csv('sample_submission.csv', index_col='id')\n",
    "df_subm['Listening_Time_minutes'] = y_pred / 5\n",
    "df_subm = df_subm.reset_index()\n",
    "df_subm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm.to_csv('Kaggel_listentime_submission.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
